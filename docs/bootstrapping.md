# Contents

- [Bootstrapping](#bootstrapping)
  - [Preparation](#preparation)
  - [Create Environment](#create-environment)
  - [Remote State](#remote-state)
  - [Database Migration](#database-migration)

## Bootstrapping

This document covers additional steps required when bootstrapping new environments.

Before continuing, be sure you have installed the [prerequisites](https://github.com/deadlysyn/terraform-keycloak-aws#prerequisites).

Automation wraps aws-vault for security. It needs [installed and configured](https://github.com/99designs/aws-vault#quick-start),
so be sure you have a working AWS CLI profile and have imported credentials.
Once CLIs are installed and you have an AWS profile ready, export the following
(or use something like [direnv](https://direnv.net) and a top-level `.envrc`
to export automatically):

```console
export AWS_REGION="<aws_region>"
export AWS_PROFILE="<aws_profile>"
```

The region and profile variables are used by Terraform as well
as the the build and deploy scripts.

### Preparation

The first thing to think about when preparing a new environment
is üêì and ü•ö. Before terraforming, we need environment-specific
DNS, certs, and secrets. This will be clearer if you inspect
[container_definition.json](https://github.com/deadlysyn/terraform-keycloak-aws/blob/main/modules/keycloak/templates/container_definition.json)
and
[terraform.tfvars](https://github.com/deadlysyn/terraform-keycloak-aws/blob/main/environments/template/template.tfvars).

The first thing you need is a DNS domain to host Keycloak-related
resource records. This could be a top-level domain specifically for this
purpose (e.g. `domain.tld`), or a sub-domain of an existing zone (e.g. `sso.domain.tld`).
Aside from purchasing the domain, you need to have it delegated to Route53
(when you create a Route53 hosted zone, you will be assigned a set of name servers
you can use to update the domain with your registrar). Once configured, Terraform
will manage service-specific entires in the hosted zone specified in `terraform.tfvars`.

Once a domain is configured, jump to ACM and request a cert
for the new environment. Be sure to make the request in the region where you
intend to host Keycloak, since ACM certs are region-specific. Assuming you've already
configured the hosted zone, the request process lets you create DNS validation records
with the push of a button. Wait for AWS to issue the cert, then you'll need to pass
the cert ARN via `terraform.tfvars`.

The container definition refers to `DB_PASSWORD` and `KEYCLOAK_PASSWORD`.
Both are generated by Terraform and stored in Parameter Store (this could
easily be converted to Secrets Manager or injected at runtime via your
favorite vault). The prior is the RDS admin password, and the latter is
used by the container image to set the `keycloak_admin` password on first install.

Note that generating these secrets (30 character random strings) results
in them being stored in state. This is why remote state with encryption is
enabled by default. If you prefer no secrets in state, even though they are
encrypted, you could refactor to inject secrets at runtime or
simply comment the first few lines of the keycloak module which generates
the secrets and store your own manually generated values at the same paths.

`KEYCLOAK_ADMIN` is not automatically updated when you change
the password via Keycloak's UI. If you load a database from another environment,
it will also be overwritten. To avoid confusion, keep Parameter Store in sync.

If you use the example container definition which includes a Datadog
sidecar, it also consumes a top level `/${name}/DD_API_KEY` (since one API
key can be safely shared across all environments -- adjust the path if you
prefer unique keys per environment).

### Create Environment

When bootstrapping a new environment, use `mkenv`:

```console
cd environments
./mkenv -e <env_name>
```

This will create the environment directory from a template and open an
editor with a starter set of tfvars.

There are a lot of moving parts abstracted in the `keycloak` module to
reduce cognitive load. If you need to adjust something not covered by
existing tfvars, first look at the environment specific `variables.tf`
to see if settings are being defaulted that you can override.

Failing that, the module may need expanded to cover your use case
(open a PR). The goal was to avoid exposing rarely-used settings
to simplify reasoning about environments.

### Remote State

We use [terraform-aws-tfstate-backend](https://github.com/cloudposse/terraform-aws-tfstate-backend)
to help manage remote state. For existing environments this
"just works" (when you `make update` remote state will be read and
written automatically). For a new environment, the special target
`make all` deploys the infrastructure, configures remote state
resources (S3 bucket, DynamoDB table, IAM and bucket policies),
and migrates state:

```console
cd environments/<env_name>
make all
```

Similarly, when you `make destroy`, state will be migrated
locally, state-specific resources cleaned up, and Keycloak
infrastructure destroyed (you'll have to type 'yes' to confirm).
When done, you can verify the contents of `terraform.tfstate`.

Once an environment has been destroyed, you can delete
the environment directory and start over. However, you can also simply adjust
`terraform.tfvars` as needed and then `make all` again (although if
you just want to make small changes, `make update` is sufficient).

One edge case is if Terraform encounters an error during `make all` after configuring
the S3 backend but before state has been migrated. Since `backend.tf`
will be present but state still local, you may need to `make clean`
before you can `make destroy`. This is a non-happy path that needs more testing.

For full detail on the first-time remote state setup see
[the module's instructions](https://github.com/cloudposse/terraform-aws-tfstate-backend#usage)
(you only need this if curious, or to troubleshoot if something goes wrong).

### Database Migration

When deploying a new environment, Keycloak will initialize the database.
If you need to migrate data from an existing environment, you can use the
migration scripts in the `db` directory (Postgres only at the moment).

You do not need to stop the service or database in the source
cluster. Stop the ECS tasks in the destination cluster (update deployment
to 0 tasks or they will restart). The next app deployment will restart all tasks.
On first start, Keycloak will automatically detect the schema version and apply
any needed migrations.

To run the migration scripts you'll need the following:

- Source database hostname
- Source database password
- Destination database hostname
- Destination database password

For any environments generated by this automation, you can get the
database hostnames from the RDS console and passwords from Paramter Store.
To access the databases, you'll need network connectivity between the
machine running the scripts and the database hosts.

```console
cd db
./dump.sh <src_db_hostname>
# Source DB password when prompted...
./restore.sh <dst_db_hostname> <file_created_above>
# Destination DB password when prompted... twice. :-)
```

Revoke/grant warnings at the end of the restore are normal.
